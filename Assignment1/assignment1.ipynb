{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DD2424 Assignment 1\n",
    "\n",
    "The goal of this project was to implement a simple image classifier for the CIFAR-10 dataset using softmax regression. The focus was on understanding key machine learning concepts such as data preprocessing, forward and backward propagation, gradient checking, and training using mini-batch gradient descent.\n",
    "\n",
    "I was able to successfully implement the analytical computation of gradients for the cross-entropy loss with L2 regularization in the BackwardPass function. To verify the correctness of this implementation, I compared my gradients against numerically computed gradients from PyTorch on a small subset of the data (d = 10, n = 3).\n",
    "\n",
    "The comparison was made using a relative error metric:\n",
    "\n",
    "$\\text{Relative Error} = \\frac{|g_a - g_n|}{\\max(\\epsilon, |g_a| + |g_n|)}$\n",
    "\n",
    "where $g_a$ is the analytical gradient, $g_n$ is the numerical gradient, and $\\epsilon$ is a small constant to avoid division by zero.\n",
    "\n",
    "I tested the gradients of both the weights and biases, and the max relative error in W was $\\sim$ 1.09e-15 and in b was $\\sim$ 9.78e-17. I was able to conclude that the analytical gradients are correct, as the relative errors are very small and within the acceptable range for numerical precision."
   ],
   "id": "69168ed65954e00d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q1. Loss and Cost Function plots\n",
    "\n",
    "The loss function is defined as the average negative log-likelihood of the true labels given the predicted probabilities. The cost function includes L2 regularization to prevent overfitting. The loss and cost functions are computed during training and plotted to visualize the model's performance.\n",
    "\n",
    "\n",
    "### Plot 1: Loss function\n"
   ],
   "id": "8522597730b50a0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The plot shows the training and validation loss over 40 epochs of mini-batch gradient descent. Both losses decrease steadily, indicating that the model is learning effectively. The training loss remains slightly lower than the validation loss, suggesting mild overfitting but overall good generalization.",
   "id": "6bf4929102deeac1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![loss_funtion](loss_plot.jpg)",
   "id": "ef119fcc177bf466"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Plot 2: Cost function\n",
    "\n",
    "The plot shows the training and validation cost over 40 epochs of mini-batch gradient descent, including both loss and L2 regularization. Both curves decrease consistently, indicating successful optimization. The validation cost remains slightly higher, reflecting regularization’s role in promoting generalization.\n",
    "\n",
    "\n",
    "![cost_function](cost_plot.jpg)"
   ],
   "id": "ed1d7c19cb0246f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q2. Images representing the learnt weight matrix after the completion of training\n",
    "\n",
    "W is the weight matrix of the softmax classifier, where each column corresponds to a class and each row corresponds to a pixel in the image. The images are reshaped from the weight matrix and displayed to visualize the features learned by the model.\n",
    "\n",
    "\n",
    "### Fig 1: The learnt W matrix visualized as class template images. The network was trained with the following parameter settings: n_batch=100, eta=.1, n_epochs=40 and lam=0.\n",
    "\n",
    "![Fig 1](filters_all_in_one1.png)\n"
   ],
   "id": "3e3608caa3d8d269"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The image above shows the visualization of the learned weight matrix W after training the network for 40 epochs using mini-batch gradient descent. Each square represents the weight filter for one of the 10 CIFAR-10 classes, reshaped into a 32×32×3 RGB image. These templates highlight the patterns the model associates with each class based on the training data, using the following settings: n_batch = 100, eta = 0.001, n_epochs = 40, and lam = 0.\n",
    "\n",
    "After training the network, the training accuracy after first epoch was 26.72% and the test accuracy was 29.77%. The training accuracy after 40 epochs was 39.48% and the test accuracy was 43.31%. The model's performance improved over time, indicating effective learning from the CIFAR-10 dataset.\n",
    "\n",
    "### Fig2: The learnt W matrix visualized as class template images. The network was trained with the following parameter settings: n_batch=100, eta=.001, n_epochs=40 and lam=0.\n",
    "\n",
    "![Fig 2](filters_all_in_one2.png)\n",
    "\n",
    "After training the network, the training accuracy after first epoch was 30% and the test accuracy was 30.72%. The training accuracy after 40 epochs was 45.17% and the test accuracy was 45.59%. The model's performance improved over time, indicating effective learning from the CIFAR-10 dataset."
   ],
   "id": "8f6334fbd1bc86f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fig3: The learnt W matrix visualized as class template images. The network was trained with the following parameter settings: n_batch=100, eta=.001, n_epochs=40 and lam=0.1\n",
    "\n",
    "![Fig 3](filters_all_in_one3.png)\n",
    "\n"
   ],
   "id": "8aa4c5ae70f9b5d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After training the network, the training accuracy after first epoch was 30.07% and the test accuracy was 30.76%. The training accuracy after 40 epochs was 44.87% and the test accuracy was 44.72%. The model's performance improved over time, indicating effective learning from the CIFAR-10 dataset.\n",
    "\n",
    "### Fig4: The learnt W matrix visualized as class template images. The network was trained with the following parameter settings: n_batch=100, eta=.001, n_epochs=40 and lam=1\n",
    "\n",
    "![Fig 4](filters_all_in_one4.png)"
   ],
   "id": "b71c0d34093076c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After training the network, the training accuracy after first epoch was 30.3% and the test accuracy was 31.01%. The training accuracy after 40 epochs was 40% and the test accuracy was 38.99%. The model's performance improved over time, indicating effective learning from the CIFAR-10 dataset.",
   "id": "b1076f77c487c216"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We see from above four results that increasing the regularisation parameter lam helps us reduce overfitting, improving generalization. However, increasing it too much can lead to underfitting. Here, the best performance was achieved with lam = 0, although lam = 0.1 was also effective. The model's performance improved over time, indicating effective learning from the CIFAR-10 dataset. It is important to balance the regularization parameter to achieve optimal performance.\n",
    "\n",
    "The learning rate eta also plays a crucial role in the training process, as it controls the step sizes of the updates during training. A smaller learning rate may lead to slower convergence and might get stuck. In this case, eta = 0.001 was effective, and eta = 0.1 was too large, leading to divergence. Finding the correct learning rate is key to successful training."
   ],
   "id": "d05e63481020b925"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "14b30209c46579a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
